{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis Notebook\n",
        "\n",
        "Use Python code cells to run analyses inline.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "print(\"Notebook ready\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speed Dating → Logit Utilities → Gale–Shapley Stable Matching\n",
        "\n",
        "This notebook is a **fully worked end-to-end exercise** that links:\n",
        "\n",
        "1. A **logit model** for individual yes/no decisions (utility estimation)\n",
        "2. **Predicted probabilities** to build **preference rankings**\n",
        "3. The **Gale–Shapley** algorithm to compute **stable matchings**\n",
        "4. A comparison to the observed **`match`** outcomes in the data\n",
        "\n",
        "**Data file:** `../speed-dating.csv`\n",
        "\n",
        "> Note on IDs: this cleaned dataset does not include explicit participant IDs.\n",
        "> We reconstruct consistent IDs within each `wave` using stable demographic variables\n",
        "> and the stated preference-weight questions.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Setup ---\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Make matplotlib work in restricted environments (no writable home cache)\n",
        "os.environ.setdefault('MPLCONFIGDIR', str(Path('.latex-interface') / 'mpl-cache'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "RANDOM_SEED = 0\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Load the data\n",
        "\n",
        "Each row is one speed-date **from one participant's perspective**.\n",
        "Key variables we will use:\n",
        "\n",
        "- `decision`: whether the participant wants to see the partner again (0/1)\n",
        "- `decision_o`: the partner's decision (0/1)\n",
        "- `match`: 1 if both said yes\n",
        "- `wave`: a separate speed-dating market/event\n",
        "- `gender`: participant gender (male/female)\n",
        "\n",
        "We will use `wave = 1` as the running example (10 men, 10 women → 100 pairs).\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RAW_DATA_PATH = Path('../speed-dating.csv')\n",
        "PROCESSED_DATA_PATH = Path('data/speed-dating-ethnicity-with-ids.csv')\n",
        "\n",
        "# If the processed file exists (the shareable student version), use it.\n",
        "# Otherwise fall back to the raw file and create the processed dataset later.\n",
        "if PROCESSED_DATA_PATH.exists():\n",
        "    df = pd.read_csv(PROCESSED_DATA_PATH)\n",
        "    print('Loaded processed dataset:', PROCESSED_DATA_PATH)\n",
        "else:\n",
        "    assert RAW_DATA_PATH.exists(), f\"Missing raw data file: {RAW_DATA_PATH.resolve()}\"\n",
        "    df = pd.read_csv(RAW_DATA_PATH)\n",
        "\n",
        "    # Rename race-related variables to ethnicity-related names\n",
        "    df = df.rename(columns={\n",
        "        'race': 'ethnicity',\n",
        "        'race_o': 'ethnicity_o',\n",
        "        'samerace': 'same_ethnicity',\n",
        "        'importance_same_race': 'importance_same_ethnicity',\n",
        "        'd_importance_same_race': 'd_importance_same_ethnicity',\n",
        "    })\n",
        "    print('Loaded raw dataset:', RAW_DATA_PATH)\n",
        "\n",
        "print('Shape:', df.shape)\n",
        "print('Columns:', len(df.columns))\n",
        "df.head(3)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity checks\n",
        "print(df['gender'].value_counts(dropna=False))\n",
        "print('Waves:', df['wave'].nunique())\n",
        "print('Decision mean:', df['decision'].mean())\n",
        "print('Match mean:', df['match'].mean())\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.1) Reconstruct participant IDs\n",
        "\n",
        "To run Gale–Shapley we need consistent IDs for the two sides of the market.\n",
        "\n",
        "This dataset contains **stable preference-weight questions** (they sum to 100):\n",
        "\n",
        "- Participant's own weights: `*_important`\n",
        "- Partner's weights (same questions but for the partner): `pref_o_*`\n",
        "\n",
        "So we can identify a person within a wave by:\n",
        "\n",
        "- `wave`, `gender`, `age`, `ethnicity`\n",
        "- their preference-weight vector (`*_important`)\n",
        "\n",
        "And we can identify the partner in the row by the *same* information using\n",
        "`age_o`, `ethnicity_o`, and `pref_o_*`.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imp_cols = [\n",
        "    'attractive_important',\n",
        "    'sincere_important',\n",
        "    'intellicence_important',\n",
        "    'funny_important',\n",
        "    'ambtition_important',\n",
        "    'shared_interests_important',\n",
        "]\n",
        "\n",
        "pref_cols = [\n",
        "    'pref_o_attractive',\n",
        "    'pref_o_sincere',\n",
        "    'pref_o_intelligence',\n",
        "    'pref_o_funny',\n",
        "    'pref_o_ambitious',\n",
        "    'pref_o_shared_interests',\n",
        "]\n",
        "\n",
        "# If the processed dataset is loaded, it already has person_id/partner_id.\n",
        "# If not, reconstruct IDs from stable demographics + preference-weight vectors.\n",
        "df = df.copy()\n",
        "\n",
        "if 'person_id' not in df.columns or 'partner_id' not in df.columns:\n",
        "    # Respondent signature → person_id\n",
        "    person_sig_cols = ['wave', 'gender', 'age', 'ethnicity'] + imp_cols\n",
        "    df['person_sig'] = df[person_sig_cols].astype(str).agg('|'.join, axis=1)\n",
        "    df['person_id'] = pd.factorize(df['person_sig'])[0]\n",
        "\n",
        "    sig_to_id = (\n",
        "        df[['person_sig', 'person_id']]\n",
        "        .drop_duplicates('person_sig')\n",
        "        .set_index('person_sig')['person_id']\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    # Partner signature → partner_id (mapped into the same ID space)\n",
        "    partner_gender = df['gender'].map({'male': 'female', 'female': 'male'})\n",
        "    partner_sig_df = pd.DataFrame({\n",
        "        'wave': df['wave'],\n",
        "        'gender': partner_gender,\n",
        "        'age': df['age_o'],\n",
        "        'ethnicity': df['ethnicity_o'],\n",
        "    })\n",
        "    for imp, pref in zip(imp_cols, pref_cols):\n",
        "        partner_sig_df[imp] = df[pref]\n",
        "\n",
        "    df['partner_sig'] = partner_sig_df.astype(str).agg('|'.join, axis=1)\n",
        "    df['partner_id'] = df['partner_sig'].map(sig_to_id)\n",
        "else:\n",
        "    # Ensure consistent dtypes when loading from CSV\n",
        "    df['person_id'] = df['person_id'].astype(int)\n",
        "    df['partner_id'] = df['partner_id'].astype(int)\n",
        "\n",
        "print('Rows with missing partner_id:', df['partner_id'].isna().sum())\n",
        "print('Unique participants (reconstructed):', df[['wave','gender','person_id']].drop_duplicates().shape[0])\n",
        "\n",
        "# Create the analysis dataset we share with students: keep only rows with both IDs\n",
        "# and save it with ethnicity terminology.\n",
        "df_clean = df.dropna(subset=['person_id', 'partner_id']).copy()\n",
        "df_clean['person_id'] = df_clean['person_id'].astype(int)\n",
        "df_clean['partner_id'] = df_clean['partner_id'].astype(int)\n",
        "\n",
        "PROCESSED_DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_clean.to_csv(PROCESSED_DATA_PATH, index=False)\n",
        "print('Wrote processed analysis dataset:', PROCESSED_DATA_PATH)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Participants per wave (reconstructed)\n",
        "people = df[['wave','gender','person_id','age','ethnicity']].drop_duplicates(['wave','gender','person_id'])\n",
        "counts = people.groupby(['wave','gender']).size().unstack(fill_value=0)\n",
        "counts['balanced'] = (counts.get('female',0) == counts.get('male',0))\n",
        "counts.sort_values(['balanced', 'female'], ascending=[False, False]).head(12)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.2) Choose a wave (a matching market)\n",
        "\n",
        "We will work with one wave as a clean two-sided matching market.\n",
        "You can change `WAVE` to explore other markets.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WAVE = 1\n",
        "\n",
        "# Keep only rows where both sides have reconstructed IDs\n",
        "df_clean = df.dropna(subset=['person_id', 'partner_id']).copy()\n",
        "# partner_id is created via a mapping (float + NaN); after dropping NaNs we cast to int\n",
        "df_clean['person_id'] = df_clean['person_id'].astype(int)\n",
        "df_clean['partner_id'] = df_clean['partner_id'].astype(int)\n",
        "df_wave = df_clean[df_clean['wave'] == WAVE].copy()\n",
        "\n",
        "print('Wave rows:', df_wave.shape[0])\n",
        "print('Unique women:', df_wave[df_wave['gender']=='female']['person_id'].nunique())\n",
        "print('Unique men  :', df_wave[df_wave['gender']=='male']['person_id'].nunique())\n",
        "\n",
        "# Create short labels for nicer tables/plots\n",
        "men_ids = sorted(df_wave[df_wave['gender']=='male']['person_id'].unique().tolist())\n",
        "women_ids = sorted(df_wave[df_wave['gender']=='female']['person_id'].unique().tolist())\n",
        "\n",
        "id_to_label = {pid: f\"M{idx+1:02d}\" for idx, pid in enumerate(men_ids)}\n",
        "id_to_label.update({pid: f\"W{idx+1:02d}\" for idx, pid in enumerate(women_ids)})\n",
        "\n",
        "people_wave = (\n",
        "    people[people['wave'] == WAVE]\n",
        "    .assign(label=lambda d: d['person_id'].map(id_to_label))\n",
        "    .sort_values(['gender','label'])\n",
        ")\n",
        "people_wave.head(10)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Step 1 — Estimate utilities with a logit model\n",
        "\n",
        "We model the probability that participant *i* says yes to partner *j* as:\n",
        "\n",
        "\\[\\Pr(\texttt{decision}_{ij}=1\\mid X_{ij}) = \\Lambda(X_{ij}'\beta)\\]\n",
        "\n",
        "where \\(\\Lambda(\\cdot)\\) is the logistic CDF.\n",
        "\n",
        "- The *linear index* \\(U_{ij} = X_{ij}'\beta\\) is an empirical **utility score**.\n",
        "- The predicted probability \\(\\hat p_{ij}\\) is a convenient **ranking score**.\n",
        "\n",
        "We estimate **separate models** for women and men (preferences may differ).\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURES = [\n",
        "    'attractive_o',\n",
        "    'sinsere_o',\n",
        "    'intelligence_o',\n",
        "    'funny_o',\n",
        "    'ambitous_o',\n",
        "    'shared_interests_o',\n",
        "    'd_age',\n",
        "    'same_ethnicity',\n",
        "]\n",
        "TARGET = 'decision'\n",
        "\n",
        "# We fit on the full dataset for stable coefficients, then apply to a wave.\n",
        "\n",
        "def fit_logit_for_gender(df_in, gender):\n",
        "    df_g = df_in[df_in['gender'] == gender].copy()\n",
        "    df_g = df_g.dropna(subset=[TARGET])\n",
        "\n",
        "    X = df_g[FEATURES]\n",
        "    y = df_g[TARGET].astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=RANDOM_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('logit', LogisticRegression(max_iter=1000, solver='lbfgs')),\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    p_test = model.predict_proba(X_test)[:, 1]\n",
        "    yhat = (p_test >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        'gender': gender,\n",
        "        'n_train': len(X_train),\n",
        "        'n_test': len(X_test),\n",
        "        'auc': roc_auc_score(y_test, p_test),\n",
        "        'accuracy@0.5': accuracy_score(y_test, yhat),\n",
        "        'base_rate': float(y_test.mean()),\n",
        "    }\n",
        "    return model, metrics\n",
        "\n",
        "models = {}\n",
        "metrics = []\n",
        "for g in ['female', 'male']:\n",
        "    m, met = fit_logit_for_gender(df_clean, g)\n",
        "    models[g] = m\n",
        "    metrics.append(met)\n",
        "\n",
        "pd.DataFrame(metrics)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coefficients (regularized MLE coefficients)\n",
        "\n",
        "def coef_table(pipeline):\n",
        "    logit = pipeline.named_steps['logit']\n",
        "    coefs = pd.Series(logit.coef_.ravel(), index=FEATURES, name='coef')\n",
        "    out = coefs.to_frame().reset_index().rename(columns={'index': 'feature'})\n",
        "    out.loc[len(out)] = ['intercept', float(logit.intercept_[0])]\n",
        "    return out\n",
        "\n",
        "coef_female = coef_table(models['female'])\n",
        "coef_male = coef_table(models['male'])\n",
        "\n",
        "coef_female\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add predicted utility (linear index) and probability to every row\n",
        "\n",
        "df_pred = df_clean.copy()\n",
        "df_pred['utility_hat'] = np.nan\n",
        "df_pred['p_hat'] = np.nan\n",
        "\n",
        "for g, model in models.items():\n",
        "    mask = df_pred['gender'] == g\n",
        "    Xg = df_pred.loc[mask, FEATURES]\n",
        "    df_pred.loc[mask, 'utility_hat'] = model.decision_function(Xg)\n",
        "    df_pred.loc[mask, 'p_hat'] = model.predict_proba(Xg)[:, 1]\n",
        "\n",
        "print('p_hat range:', float(df_pred['p_hat'].min()), float(df_pred['p_hat'].max()))\n",
        "df_pred[['wave','gender','decision','p_hat','utility_hat']].head(5)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Step 2 — Build preference rankings and the Gale–Shapley matrices\n",
        "\n",
        "For a given wave, we can build (estimated) preferences by sorting partners by \\(\\hat p_{ij}\\).\n",
        "\n",
        "- Men’s preference list over women: sort women by men’s predicted probabilities\n",
        "- Women’s preference list over men: sort men by women’s predicted probabilities\n",
        "\n",
        "We will construct:\n",
        "\n",
        "- A men→women probability matrix \\(P^{M}\\)\n",
        "- A women→men probability matrix \\(P^{W}\\)\n",
        "- Two preference dictionaries (one for each side) to feed into Gale–Shapley\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wave_pred = df_pred[df_pred['wave'] == WAVE].copy()\n",
        "\n",
        "# Men as respondents: p(m says yes to w)\n",
        "men_rows = df_wave_pred[df_wave_pred['gender'] == 'male'].copy()\n",
        "P_m = men_rows.pivot_table(index='person_id', columns='partner_id', values='p_hat', aggfunc='mean')\n",
        "\n",
        "# Women as respondents: p(w says yes to m)\n",
        "women_rows = df_wave_pred[df_wave_pred['gender'] == 'female'].copy()\n",
        "P_w = women_rows.pivot_table(index='person_id', columns='partner_id', values='p_hat', aggfunc='mean')\n",
        "\n",
        "print('P_m shape (men x women):', P_m.shape)\n",
        "print('P_w shape (women x men):', P_w.shape)\n",
        "\n",
        "# Relabel for nicer display\n",
        "P_m_labeled = P_m.rename(index=id_to_label, columns=id_to_label).sort_index().sort_index(axis=1)\n",
        "P_w_labeled = P_w.rename(index=id_to_label, columns=id_to_label).sort_index().sort_index(axis=1)\n",
        "\n",
        "P_m_labeled.iloc[:5, :5]\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preference lists (highest probability = best)\n",
        "\n",
        "def prefs_from_matrix(P):\n",
        "    prefs = {}\n",
        "    for i, row in P.iterrows():\n",
        "        prefs[i] = row.sort_values(ascending=False).index.tolist()\n",
        "    return prefs\n",
        "\n",
        "prefs_men = prefs_from_matrix(P_m)\n",
        "prefs_women = prefs_from_matrix(P_w)\n",
        "\n",
        "first_man = men_ids[0]\n",
        "first_woman = women_ids[0]\n",
        "print('Example man', id_to_label[first_man], 'top-5 women:', [id_to_label[x] for x in prefs_men[first_man][:5]])\n",
        "print('Example woman', id_to_label[first_woman], 'top-5 men :', [id_to_label[x] for x in prefs_women[first_woman][:5]])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: visualize men's probability matrix as a heatmap\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(P_m_labeled, cmap='viridis', cbar_kws={'label': 'p_hat (man says yes)'})\n",
        "plt.title(f'Wave {WAVE}: Men → Women predicted probabilities')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Step 3 — Gale–Shapley stable matching\n",
        "\n",
        "We now compute the stable matching equilibrium under two regimes:\n",
        "\n",
        "- **Women propose** (women-optimal stable matching)\n",
        "- **Men propose** (men-optimal stable matching)\n",
        "\n",
        "Gale–Shapley guarantees a stable matching (no blocking pair) given complete preference lists.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gale_shapley(proposer_prefs, acceptor_prefs):\n",
        "    # Return a stable matching as {proposer: acceptor}\n",
        "\n",
        "    acceptor_rank = {\n",
        "        a: {p: r for r, p in enumerate(pref_list)}\n",
        "        for a, pref_list in acceptor_prefs.items()\n",
        "    }\n",
        "\n",
        "    free = list(proposer_prefs.keys())\n",
        "    next_idx = {p: 0 for p in proposer_prefs}\n",
        "    engaged = {}  # acceptor -> proposer\n",
        "\n",
        "    while free:\n",
        "        p = free.pop(0)\n",
        "        prefs = proposer_prefs[p]\n",
        "\n",
        "        if next_idx[p] >= len(prefs):\n",
        "            continue\n",
        "\n",
        "        a = prefs[next_idx[p]]\n",
        "        next_idx[p] += 1\n",
        "\n",
        "        if a not in engaged:\n",
        "            engaged[a] = p\n",
        "            continue\n",
        "\n",
        "        current = engaged[a]\n",
        "        rank_new = acceptor_rank[a].get(p, float('inf'))\n",
        "        rank_cur = acceptor_rank[a].get(current, float('inf'))\n",
        "\n",
        "        if rank_new < rank_cur:\n",
        "            engaged[a] = p\n",
        "            free.append(current)\n",
        "        else:\n",
        "            free.append(p)\n",
        "\n",
        "    match = {p: None for p in proposer_prefs}\n",
        "    for a, p in engaged.items():\n",
        "        match[p] = a\n",
        "    return match\n",
        "\n",
        "\n",
        "def matching_to_frame(match, proposer_label='proposer', acceptor_label='acceptor'):\n",
        "    rows = []\n",
        "    for p, a in match.items():\n",
        "        rows.append({\n",
        "            proposer_label: p,\n",
        "            acceptor_label: a,\n",
        "            proposer_label + '_label': id_to_label.get(p, str(p)),\n",
        "            acceptor_label + '_label': id_to_label.get(a, str(a)) if a is not None else None,\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def rank_in_prefs(prefs_dict, agent, partner):\n",
        "    if partner is None:\n",
        "        return None\n",
        "    try:\n",
        "        return prefs_dict[agent].index(partner) + 1\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# Women propose\n",
        "match_women_propose = gale_shapley(prefs_women, prefs_men)\n",
        "res_wp = matching_to_frame(match_women_propose, proposer_label='woman', acceptor_label='man')\n",
        "res_wp['woman_rank_of_man'] = res_wp.apply(lambda r: rank_in_prefs(prefs_women, r['woman'], r['man']), axis=1)\n",
        "res_wp['man_rank_of_woman'] = res_wp.apply(lambda r: rank_in_prefs(prefs_men, r['man'], r['woman']), axis=1)\n",
        "\n",
        "# Men propose\n",
        "match_men_propose = gale_shapley(prefs_men, prefs_women)\n",
        "res_mp = matching_to_frame(match_men_propose, proposer_label='man', acceptor_label='woman')\n",
        "res_mp['man_rank_of_woman'] = res_mp.apply(lambda r: rank_in_prefs(prefs_men, r['man'], r['woman']), axis=1)\n",
        "res_mp['woman_rank_of_man'] = res_mp.apply(lambda r: rank_in_prefs(prefs_women, r['woman'], r['man']), axis=1)\n",
        "\n",
        "print('Women-propose matching (first 10 rows):')\n",
        "display(res_wp.head(10))\n",
        "print('Men-propose matching (first 10 rows):')\n",
        "display(res_mp.head(10))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Welfare-style summary: average ranks (lower is better)\n",
        "\n",
        "def summarize_matching(res, proposer_side):\n",
        "    if proposer_side == 'women':\n",
        "        return pd.Series({\n",
        "            'avg woman rank (of man)': res['woman_rank_of_man'].mean(),\n",
        "            'avg man rank (of woman)': res['man_rank_of_woman'].mean(),\n",
        "        })\n",
        "    if proposer_side == 'men':\n",
        "        return pd.Series({\n",
        "            'avg man rank (of woman)': res['man_rank_of_woman'].mean(),\n",
        "            'avg woman rank (of man)': res['woman_rank_of_man'].mean(),\n",
        "        })\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'women propose': summarize_matching(res_wp, 'women'),\n",
        "    'men propose': summarize_matching(res_mp, 'men'),\n",
        "})\n",
        "summary\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Step 4 — Compare to the observed `match` outcomes\n",
        "\n",
        "`match = 1` means both sides said yes after their speed-date.\n",
        "\n",
        "Important conceptual note:\n",
        "\n",
        "- The experiment allows **many matches per person** (a person can match with multiple partners).\n",
        "- Gale–Shapley produces a **one-to-one** stable matching.\n",
        "\n",
        "So we compare in two ways:\n",
        "\n",
        "1. **Overlap with mutual matches:** Is the GS partner also a mutual match (`match=1`)?\n",
        "2. **Best one-to-one matching inside the mutual-match network:** maximum-cardinality bipartite matching using only edges with `match=1`.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a unique (man,woman) edge table from male-respondent rows\n",
        "edges = (\n",
        "    men_rows[['person_id','partner_id','match']]\n",
        "    .rename(columns={'person_id':'man_id','partner_id':'woman_id'})\n",
        "    .drop_duplicates(['man_id','woman_id'])\n",
        ")\n",
        "edge_match = edges.set_index(['man_id','woman_id'])['match']\n",
        "\n",
        "\n",
        "def overlap_with_match(df_pairs):\n",
        "    flags = []\n",
        "    for m, w in df_pairs[['man_id','woman_id']].itertuples(index=False, name=None):\n",
        "        flags.append(int(edge_match.get((m, w), 0) == 1))\n",
        "    return sum(flags), len(flags), sum(flags)/len(flags) if flags else float('nan')\n",
        "\n",
        "wp_pairs = res_wp.dropna(subset=['man']).rename(columns={'man':'man_id','woman':'woman_id'})[['man_id','woman_id']]\n",
        "mp_pairs = res_mp.dropna(subset=['woman']).rename(columns={'man':'man_id','woman':'woman_id'})[['man_id','woman_id']]\n",
        "\n",
        "wp_hit, wp_n, wp_rate = overlap_with_match(wp_pairs)\n",
        "mp_hit, mp_n, mp_rate = overlap_with_match(mp_pairs)\n",
        "\n",
        "print(f\"Wave {WAVE}: GS women-propose overlap: {wp_hit}/{wp_n} = {wp_rate:.2%}\")\n",
        "print(f\"Wave {WAVE}: GS men-propose   overlap: {mp_hit}/{mp_n} = {mp_rate:.2%}\")\n",
        "print('Total mutual matches in wave (edges with match=1):', int(edges['match'].sum()))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Maximum-cardinality one-to-one matching using only mutual-match edges\n",
        "B = nx.Graph()\n",
        "\n",
        "men_nodes = [f\"M_{m}\" for m in men_ids]\n",
        "women_nodes = [f\"W_{w}\" for w in women_ids]\n",
        "B.add_nodes_from(men_nodes, bipartite=0)\n",
        "B.add_nodes_from(women_nodes, bipartite=1)\n",
        "\n",
        "for (m, w), val in edge_match.items():\n",
        "    if val == 1:\n",
        "        B.add_edge(f\"M_{m}\", f\"W_{w}\")\n",
        "\n",
        "mm = nx.algorithms.bipartite.matching.maximum_matching(B, top_nodes=men_nodes)\n",
        "\n",
        "mm_pairs = set()\n",
        "for u, v in mm.items():\n",
        "    if u.startswith('M_') and v.startswith('W_'):\n",
        "        man_id = int(u.split('_',1)[1])\n",
        "        woman_id = int(v.split('_',1)[1])\n",
        "        mm_pairs.add((man_id, woman_id))\n",
        "\n",
        "print('Max-cardinality matching size inside mutual matches:', len(mm_pairs))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare GS pair-sets to the max-cardinality matching from observed mutual matches\n",
        "\n",
        "def pair_set(df_pairs):\n",
        "    return set(map(tuple, df_pairs[['man_id','woman_id']].values.tolist()))\n",
        "\n",
        "S_wp = pair_set(wp_pairs)\n",
        "S_mp = pair_set(mp_pairs)\n",
        "S_mm = mm_pairs\n",
        "\n",
        "mutual_edges = set(edge_match[edge_match == 1].index.tolist())\n",
        "\n",
        "def jaccard(A, B):\n",
        "    if not A and not B:\n",
        "        return 1.0\n",
        "    return len(A & B) / len(A | B)\n",
        "\n",
        "comparison = pd.DataFrame([\n",
        "    {\n",
        "        'matching': 'GS (women propose)',\n",
        "        'pairs': len(S_wp),\n",
        "        'GS_pairs_that_are_mutual_matches': len(S_wp & mutual_edges),\n",
        "        'jaccard_with_max_mutual_matching': jaccard(S_wp, S_mm),\n",
        "    },\n",
        "    {\n",
        "        'matching': 'GS (men propose)',\n",
        "        'pairs': len(S_mp),\n",
        "        'GS_pairs_that_are_mutual_matches': len(S_mp & mutual_edges),\n",
        "        'jaccard_with_max_mutual_matching': jaccard(S_mp, S_mm),\n",
        "    },\n",
        "    {\n",
        "        'matching': 'Max matching within match=1 edges',\n",
        "        'pairs': len(S_mm),\n",
        "        'GS_pairs_that_are_mutual_matches': len(S_mm),\n",
        "        'jaccard_with_max_mutual_matching': 1.0,\n",
        "    },\n",
        "])\n",
        "comparison\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Export tables/figures for LaTeX\n",
        "\n",
        "We export a few key tables and figures into `tables/` and `images/` so a LaTeX document can include them.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Path('tables').mkdir(exist_ok=True)\n",
        "Path('images').mkdir(exist_ok=True)\n",
        "\n",
        "participants_out = people_wave[['label','gender','age','ethnicity']].copy()\n",
        "participants_out.to_csv('tables/wave_participants.csv', index=False)\n",
        "participants_out.to_latex('tables/wave_participants.tex', index=False, escape=True)\n",
        "\n",
        "\n",
        "# Logit fit metrics\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df.to_csv('tables/logit_metrics.csv', index=False)\n",
        "metrics_df.to_latex('tables/logit_metrics.tex', index=False, escape=True, float_format='%.3f')\n",
        "\n",
        "coef_female.to_csv('tables/logit_coef_female.csv', index=False)\n",
        "coef_male.to_csv('tables/logit_coef_male.csv', index=False)\n",
        "coef_female.to_latex('tables/logit_coef_female.tex', index=False, escape=True)\n",
        "coef_male.to_latex('tables/logit_coef_male.tex', index=False, escape=True)\n",
        "\n",
        "wp_out = res_wp[['woman_label','man_label','woman_rank_of_man','man_rank_of_woman']].sort_values('woman_label')\n",
        "mp_out = res_mp[['man_label','woman_label','man_rank_of_woman','woman_rank_of_man']].sort_values('man_label')\n",
        "\n",
        "wp_out.to_csv('tables/gs_women_propose.csv', index=False)\n",
        "mp_out.to_csv('tables/gs_men_propose.csv', index=False)\n",
        "wp_out.to_latex('tables/gs_women_propose.tex', index=False, escape=True)\n",
        "mp_out.to_latex('tables/gs_men_propose.tex', index=False, escape=True)\n",
        "\n",
        "summary.to_latex('tables/gs_rank_summary.tex', escape=True)\n",
        "comparison_pretty = comparison.rename(columns={\n",
        "    'matching': 'Matching',\n",
        "    'pairs': 'Pairs',\n",
        "    'GS_pairs_that_are_mutual_matches': 'Pairs with match=1',\n",
        "    'jaccard_with_max_mutual_matching': 'Jaccard vs benchmark',\n",
        "})\n",
        "comparison_pretty.to_latex(\n",
        "    'tables/gs_vs_match_comparison.tex',\n",
        "    index=False,\n",
        "    escape=True,\n",
        "    float_format='%.3f',\n",
        ")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(P_m_labeled, cmap='viridis', cbar_kws={'label': 'p_hat (man says yes)'})\n",
        "plt.title(f'Wave {WAVE}: Men → Women predicted probabilities')\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/prob_matrix_men.png', dpi=200)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Preference (ranking) matrices for Gale–Shapley (as labels)\n",
        "def prefs_table(prefs_dict, agent_ids, partner_prefix):\n",
        "    rows = []\n",
        "    for a in agent_ids:\n",
        "        ranked = [id_to_label[p] for p in prefs_dict[a]]\n",
        "        rows.append([id_to_label[a]] + ranked)\n",
        "    cols = ['agent'] + [f'rank_{k:02d}' for k in range(1, len(rows[0]))]\n",
        "    return pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "prefs_men_table = prefs_table(prefs_men, men_ids, 'W')\n",
        "prefs_women_table = prefs_table(prefs_women, women_ids, 'M')\n",
        "\n",
        "prefs_men_table.to_csv('tables/prefs_men.csv', index=False)\n",
        "prefs_women_table.to_csv('tables/prefs_women.csv', index=False)\n",
        "\n",
        "# Keep LaTeX tables compact: show top-5 only\n",
        "prefs_men_table.head(10)[['agent','rank_01','rank_02','rank_03','rank_04','rank_05']].to_latex(\n",
        "    'tables/prefs_men_top5.tex', index=False, escape=True\n",
        ")\n",
        "prefs_women_table.head(10)[['agent','rank_01','rank_02','rank_03','rank_04','rank_05']].to_latex(\n",
        "    'tables/prefs_women_top5.tex', index=False, escape=True\n",
        ")\n",
        "\n",
        "# Probability matrices\n",
        "P_m_labeled.to_csv('tables/prob_matrix_men.csv')\n",
        "P_w_labeled.to_csv('tables/prob_matrix_women.csv')\n",
        "\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# Wave 9: export the same key outputs\n",
        "# (Wave 1 can have a unique stable matching; Wave 9 is a good example where the\n",
        "# women-propose and men-propose stable matchings can differ.)\n",
        "# -----------------\n",
        "WAVE_9 = 9\n",
        "\n",
        "df_wave_pred_9 = df_pred[df_pred['wave'] == WAVE_9].copy()\n",
        "men_rows_9 = df_wave_pred_9[df_wave_pred_9['gender'] == 'male'].copy()\n",
        "women_rows_9 = df_wave_pred_9[df_wave_pred_9['gender'] == 'female'].copy()\n",
        "\n",
        "men_ids_9 = sorted(men_rows_9['person_id'].unique().tolist())\n",
        "women_ids_9 = sorted(women_rows_9['person_id'].unique().tolist())\n",
        "\n",
        "id_to_label_9 = {pid: f\"M{idx+1:02d}\" for idx, pid in enumerate(men_ids_9)}\n",
        "id_to_label_9.update({pid: f\"W{idx+1:02d}\" for idx, pid in enumerate(women_ids_9)})\n",
        "\n",
        "people_wave_9 = (\n",
        "    people[people['wave'] == WAVE_9]\n",
        "    .assign(label=lambda d: d['person_id'].map(id_to_label_9))\n",
        "    .sort_values(['gender', 'label'])\n",
        ")\n",
        "\n",
        "participants_out_9 = people_wave_9[['label', 'gender', 'age', 'ethnicity']].copy()\n",
        "participants_out_9.to_csv('tables/wave09_participants.csv', index=False)\n",
        "participants_out_9.to_latex('tables/wave09_participants.tex', index=False, escape=True)\n",
        "\n",
        "# Preference probability matrices (Wave 9)\n",
        "P_m_9 = men_rows_9.pivot_table(index='person_id', columns='partner_id', values='p_hat', aggfunc='mean')\n",
        "P_w_9 = women_rows_9.pivot_table(index='person_id', columns='partner_id', values='p_hat', aggfunc='mean')\n",
        "\n",
        "prefs_men_9 = prefs_from_matrix(P_m_9)\n",
        "prefs_women_9 = prefs_from_matrix(P_w_9)\n",
        "\n",
        "# Gale–Shapley (Wave 9)\n",
        "match_women_propose_9 = gale_shapley(prefs_women_9, prefs_men_9)\n",
        "res_wp_9 = pd.DataFrame({'woman': list(match_women_propose_9.keys()), 'man': list(match_women_propose_9.values())})\n",
        "res_wp_9['woman_label'] = res_wp_9['woman'].map(id_to_label_9)\n",
        "res_wp_9['man_label'] = res_wp_9['man'].map(id_to_label_9)\n",
        "res_wp_9['woman_rank_of_man'] = res_wp_9.apply(lambda r: rank_in_prefs(prefs_women_9, r['woman'], r['man']), axis=1)\n",
        "res_wp_9['man_rank_of_woman'] = res_wp_9.apply(lambda r: rank_in_prefs(prefs_men_9, r['man'], r['woman']), axis=1)\n",
        "\n",
        "match_men_propose_9 = gale_shapley(prefs_men_9, prefs_women_9)\n",
        "res_mp_9 = pd.DataFrame({'man': list(match_men_propose_9.keys()), 'woman': list(match_men_propose_9.values())})\n",
        "res_mp_9['man_label'] = res_mp_9['man'].map(id_to_label_9)\n",
        "res_mp_9['woman_label'] = res_mp_9['woman'].map(id_to_label_9)\n",
        "res_mp_9['man_rank_of_woman'] = res_mp_9.apply(lambda r: rank_in_prefs(prefs_men_9, r['man'], r['woman']), axis=1)\n",
        "res_mp_9['woman_rank_of_man'] = res_mp_9.apply(lambda r: rank_in_prefs(prefs_women_9, r['woman'], r['man']), axis=1)\n",
        "\n",
        "wp_out_9 = res_wp_9[['woman_label', 'man_label', 'woman_rank_of_man', 'man_rank_of_woman']].sort_values('woman_label')\n",
        "mp_out_9 = res_mp_9[['man_label', 'woman_label', 'man_rank_of_woman', 'woman_rank_of_man']].sort_values('man_label')\n",
        "\n",
        "wp_out_9.to_csv('tables/gs_women_propose_wave09.csv', index=False)\n",
        "mp_out_9.to_csv('tables/gs_men_propose_wave09.csv', index=False)\n",
        "wp_out_9.to_latex('tables/gs_women_propose_wave09.tex', index=False, escape=True)\n",
        "mp_out_9.to_latex('tables/gs_men_propose_wave09.tex', index=False, escape=True)\n",
        "\n",
        "summary_9 = pd.DataFrame({\n",
        "    'women propose': summarize_matching(res_wp_9, 'women'),\n",
        "    'men propose': summarize_matching(res_mp_9, 'men'),\n",
        "})\n",
        "summary_9.to_latex('tables/gs_rank_summary_wave09.tex', escape=True, float_format='%.3f')\n",
        "\n",
        "# Compare to observed match outcomes (Wave 9)\n",
        "edges_9 = (\n",
        "    men_rows_9[['person_id', 'partner_id', 'match']]\n",
        "    .rename(columns={'person_id': 'man_id', 'partner_id': 'woman_id'})\n",
        "    .drop_duplicates(['man_id', 'woman_id'])\n",
        ")\n",
        "edge_match_9 = edges_9.set_index(['man_id', 'woman_id'])['match']\n",
        "mutual_edges_9 = set(edge_match_9[edge_match_9 == 1].index.tolist())\n",
        "\n",
        "wp_pairs_9 = set(map(tuple, res_wp_9.dropna(subset=['man'])[['man', 'woman']].values.tolist()))\n",
        "mp_pairs_9 = set(map(tuple, res_mp_9.dropna(subset=['woman'])[['man', 'woman']].values.tolist()))\n",
        "\n",
        "B_9 = nx.Graph()\n",
        "men_nodes_9 = [f\"M_{m}\" for m in men_ids_9]\n",
        "women_nodes_9 = [f\"W_{w}\" for w in women_ids_9]\n",
        "B_9.add_nodes_from(men_nodes_9, bipartite=0)\n",
        "B_9.add_nodes_from(women_nodes_9, bipartite=1)\n",
        "for (m, w), val in edge_match_9.items():\n",
        "    if val == 1:\n",
        "        B_9.add_edge(f\"M_{int(m)}\", f\"W_{int(w)}\")\n",
        "\n",
        "mm_9 = nx.algorithms.bipartite.matching.maximum_matching(B_9, top_nodes=men_nodes_9)\n",
        "mm_pairs_9 = set()\n",
        "for u, v in mm_9.items():\n",
        "    if u.startswith('M_') and v.startswith('W_'):\n",
        "        mm_pairs_9.add((int(u.split('_', 1)[1]), int(v.split('_', 1)[1])))\n",
        "\n",
        "comparison_9 = pd.DataFrame([\n",
        "    {\n",
        "        'matching': 'GS (women propose)',\n",
        "        'pairs': len(wp_pairs_9),\n",
        "        'GS_pairs_that_are_mutual_matches': len(wp_pairs_9 & mutual_edges_9),\n",
        "        'jaccard_with_max_mutual_matching': jaccard(wp_pairs_9, mm_pairs_9),\n",
        "    },\n",
        "    {\n",
        "        'matching': 'GS (men propose)',\n",
        "        'pairs': len(mp_pairs_9),\n",
        "        'GS_pairs_that_are_mutual_matches': len(mp_pairs_9 & mutual_edges_9),\n",
        "        'jaccard_with_max_mutual_matching': jaccard(mp_pairs_9, mm_pairs_9),\n",
        "    },\n",
        "    {\n",
        "        'matching': 'Max matching within match=1 edges',\n",
        "        'pairs': len(mm_pairs_9),\n",
        "        'GS_pairs_that_are_mutual_matches': len(mm_pairs_9),\n",
        "        'jaccard_with_max_mutual_matching': 1.0,\n",
        "    },\n",
        "])\n",
        "\n",
        "comparison_9_pretty = comparison_9.rename(columns={\n",
        "    'matching': 'Matching',\n",
        "    'pairs': 'Pairs',\n",
        "    'GS_pairs_that_are_mutual_matches': 'Pairs with match=1',\n",
        "    'jaccard_with_max_mutual_matching': 'Jaccard vs benchmark',\n",
        "})\n",
        "comparison_9_pretty.to_latex(\n",
        "    'tables/gs_vs_match_comparison_wave09.tex',\n",
        "    index=False,\n",
        "    escape=True,\n",
        "    float_format='%.3f',\n",
        ")\n",
        "\n",
        "# Optional: Wave 9 heatmap (men → women)\n",
        "P_m_9_labeled = P_m_9.rename(index=id_to_label_9, columns=id_to_label_9).sort_index().sort_index(axis=1)\n",
        "plt.figure(figsize=(9, 8))\n",
        "sns.heatmap(P_m_9_labeled, cmap='viridis', cbar_kws={'label': 'p_hat (man says yes)'})\n",
        "plt.title(f'Wave {WAVE_9}: Men → Women predicted probabilities')\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/prob_matrix_men_wave09.png', dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print('Also wrote Wave 9 tables (suffix _wave09) and images/prob_matrix_men_wave09.png')\n",
        "\n",
        "# Package everything for students\n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "# Build the PDF from main.tex so the package is self-contained\n",
        "build_dir = Path('.latex-interface/build')\n",
        "build_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    for _ in range(2):\n",
        "        subprocess.run(\n",
        "            [\n",
        "                'pdflatex',\n",
        "                '-interaction=nonstopmode',\n",
        "                '-halt-on-error',\n",
        "                '-output-directory',\n",
        "                str(build_dir),\n",
        "                'main.tex',\n",
        "            ],\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "        )\n",
        "    print('Built PDF:', build_dir / 'main.pdf')\n",
        "except Exception as e:\n",
        "    print('WARNING: PDF build failed; continuing without rebuilding PDF.')\n",
        "    print(e)\n",
        "\n",
        "# Build a standalone Gale–Shapley explainer PDF as well\n",
        "try:\n",
        "    for _ in range(2):\n",
        "        subprocess.run(\n",
        "            [\n",
        "                'pdflatex',\n",
        "                '-interaction=nonstopmode',\n",
        "                '-halt-on-error',\n",
        "                '-output-directory',\n",
        "                str(build_dir),\n",
        "                'gale_shapley_explained.tex',\n",
        "            ],\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "        )\n",
        "    print('Built PDF:', build_dir / 'gale_shapley_explained.pdf')\n",
        "except Exception as e:\n",
        "    print('WARNING: Gale–Shapley explainer PDF build failed; continuing without it.')\n",
        "    print(e)\n",
        "\n",
        "package_path = Path('student_package_speed_dating_gs.zip')\n",
        "with zipfile.ZipFile(package_path, 'w', compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for p in [\n",
        "        Path('Initial.ipynb'),\n",
        "        Path('main.tex'),\n",
        "        Path('gale_shapley_explained.tex'),\n",
        "        Path('references.bib'),\n",
        "        PROCESSED_DATA_PATH,\n",
        "    ]:\n",
        "        if p.exists():\n",
        "            z.write(p, arcname=str(p))\n",
        "\n",
        "    for p in Path('tables').glob('*'):\n",
        "        if p.is_file():\n",
        "            z.write(p, arcname=str(p))\n",
        "\n",
        "    for p in Path('images').glob('*.png'):\n",
        "        z.write(p, arcname=str(p))\n",
        "\n",
        "    pdf = build_dir / 'main.pdf'\n",
        "    if pdf.exists():\n",
        "        z.write(pdf, arcname=str(pdf))\n",
        "\n",
        "    pdf = build_dir / 'gale_shapley_explained.pdf'\n",
        "    if pdf.exists():\n",
        "        z.write(pdf, arcname=str(pdf))\n",
        "\n",
        "print('Wrote student package:', package_path)\n",
        "\n",
        "print('Wrote tables/*.tex, tables/*.csv and images/*.png')\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}